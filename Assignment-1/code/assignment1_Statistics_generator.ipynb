{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "import re\n",
    "import pandas as pd\n",
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "df_json = pd.read_json('./comp_arg_hackers.json','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "countTokens=0\n",
    "countSentence = 0\n",
    "countEssays=0\n",
    "for essay in df_json['text']:\n",
    "    stringList=essay.split('\\n')\n",
    "    tokens = nlp(' '.join(stringList[1:]))\n",
    "    countTokens += len(tokens)\n",
    "    sentences = list(tokens.sents)\n",
    "    countSentence += len(sentences)\n",
    "    countEssays += 1\n",
    "    \n",
    "countPara=0\n",
    "for para in df_json['paragraphs']:\n",
    "    countPara += len(para)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 1: count of sentences, paragraphs, essays and tokens among all the essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of essays:  322\n",
      "number of tokens:  115883\n",
      "number of sentences:  5464\n",
      "number of paragraphs:  820\n"
     ]
    }
   ],
   "source": [
    "print('number of essays: ', countEssays)\n",
    "print('number of tokens: ', countTokens)\n",
    "print('number of sentences: ', countSentence)\n",
    "print('number of paragraphs: ', countPara)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Number of major claims, claims, premises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMajor=0\n",
    "countClaims = 0\n",
    "countPremise = 0\n",
    "for majorClaim, claim, premise in zip(df_json['major_claim'],df_json['claims'],df_json['premises']):\n",
    "    countMajor += len(majorClaim)\n",
    "    countClaims += len(claim)\n",
    "    countPremise += len(premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of major claims in all essays:  598\n",
      "number of claims in all essays:  1202\n",
      "number of premises in all essays:  3023\n"
     ]
    }
   ],
   "source": [
    "print('number of major claims in all essays: ',countMajor)\n",
    "print('number of claims in all essays: ', countClaims)\n",
    "print('number of premises in all essays: ', countPremise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Number of essays with confirmation bias and no confirmation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of essays with confirmation bias:  122\n",
      "number of essays with no confirmation bias:  200\n"
     ]
    }
   ],
   "source": [
    "print('number of essays with confirmation bias: ', len(df_json[df_json.confirmation_bias == True]))\n",
    "print('number of essays with no confirmation bias: ', len(df_json[df_json.confirmation_bias == False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Number of sufficient and insufficient paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sufficient paragraphs:  538\n",
      "number of insufficient paragraphs:  282\n"
     ]
    }
   ],
   "source": [
    "countSuff =0\n",
    "countNotSuff = 0\n",
    "for paraList in df_json['paragraphs']:\n",
    "    for para in paraList:\n",
    "        if para['sufficient'] == True:\n",
    "            countSuff += 1\n",
    "        else: countNotSuff += 1\n",
    "\n",
    "\n",
    "print('number of sufficient paragraphs: ', countSuff)\n",
    "print('number of insufficient paragraphs: ', countNotSuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will remove stopwords and spaces and lemmatise the words. \n",
    "def preProcess(text):\n",
    "    tokens = nlp(text)\n",
    "    processedList = list()\n",
    "    for token in tokens:\n",
    "        if (not token.is_punct and  \n",
    "                not token.is_space and  \n",
    "                not token.is_stop):\n",
    "            \n",
    "            processedList.append(token.lemma_)\n",
    "    return len(tokens), processedList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "countTokensMajor = 0\n",
    "countTokensClaim=0\n",
    "countTokensPremise=0\n",
    "majClaimText = \"\"\n",
    "claimText = \"\"\n",
    "premiseText = \"\"\n",
    "\n",
    "\n",
    "for majorclaims, claims, premises in zip(df_json['major_claim'],df_json['claims'],df_json['premises']):\n",
    "    for major in majorclaims:\n",
    "        majClaimText = major['text'] + \" \" + majClaimText\n",
    "    \n",
    "    for claim in claims:\n",
    "        claimText = claim['text'] + \" \" + claimText\n",
    "    \n",
    "    for premise in premises:\n",
    "        premiseText = premise['text'] + \" \" + premiseText\n",
    "    \n",
    "countTokensMajor, majorClaimLemmaList = preProcess(majClaimText.strip())\n",
    "countTokensClaim, claimLemmaList = preProcess(claimText.strip())\n",
    "countTokensPremise, premiseLemmaList = preProcess(premiseText.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Average number of tokens in major claims, claims, and premises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens in major claims:  8788\n",
      "average tokens in major claims:  14.695652173913043\n",
      "total tokens in claims:  18139\n",
      "average tokens in claims:  15.090682196339435\n",
      "total tokens in premises:  53211\n",
      "average tokens in premises:  17.60205094277208\n"
     ]
    }
   ],
   "source": [
    "print ('total tokens in major claims: ', countTokensMajor)        \n",
    "print ('average tokens in major claims: ', countTokensMajor/countMajor)\n",
    "\n",
    "print ('total tokens in claims: ', countTokensClaim)        \n",
    "print ('average tokens in claims: ', countTokensClaim/countClaims)\n",
    "\n",
    "print ('total tokens in premises: ', countTokensPremise)        \n",
    "print ('average tokens in premises: ', countTokensPremise/countPremise)\n",
    "\n",
    "majClaimLemmaJoined = \" \".join(majorClaimLemmaList)\n",
    "claimLemmaJoined = \" \".join(claimLemmaList)\n",
    "premiseLemmaJoined = \" \".join(premiseLemmaList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: specific words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the definition of TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "\n",
    "## combined List of docs\n",
    "docs = [majClaimLemmaJoined, claimLemmaJoined, premiseLemmaJoined]\n",
    "\n",
    "## term frequency word\n",
    "def termFreq(text):\n",
    "    countDict = dict()\n",
    "    tokens = text.split(' ')\n",
    "    for word in tokens:\n",
    "        if word in countDict:\n",
    "            countDict[word] += 1\n",
    "        else:\n",
    "            countDict[word] = 1\n",
    "\n",
    "    return countDict\n",
    "    \n",
    "## inverse Document\n",
    "def inversedf(word):\n",
    "    \n",
    "    count=0\n",
    "    for doc in docs:\n",
    "        if word in doc:\n",
    "            count +=1\n",
    "        \n",
    "    \n",
    "    return math.log(3/count)\n",
    "    \n",
    "def tfidf(docFor):\n",
    "    freqDict = termFreq(docFor)\n",
    "    dic = dict()\n",
    "    for word,count in freqDict.items():\n",
    "        dic[word] = inversedf(word) * count\n",
    "    \n",
    "    return dic\n",
    "    \n",
    "    \n",
    "    \n",
    "majorDic = tfidf(majClaimLemmaJoined)\n",
    "\n",
    "sortedMajorDict = dict(sorted(majorDic.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "claimDic = tfidf(claimLemmaJoined)\n",
    "sortedClaimDict = dict(sorted(claimDic.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "premiseDic = tfidf(premiseLemmaJoined)\n",
    "sortedPremiseDict = dict(sorted(premiseDic.items(), key=operator.itemgetter(1),reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collecting specific words For major Claims which are not in others\n",
    "tfListMajor =list()\n",
    "tfListMajor_word = list()\n",
    "for word,valueMajor in sortedMajorDict.items():\n",
    "    \n",
    "    if word in sortedClaimDict:\n",
    "        valueClaim = sortedClaimDict[word]\n",
    "        if valueClaim >= valueMajor:\n",
    "            continue\n",
    "    elif word in sortedPremiseDict:\n",
    "        valuePremise = sortedPremiseDict[word]\n",
    "        if valuePremise >= valueMajor:\n",
    "            continue\n",
    "        \n",
    "    tfListMajor.append([word,valueMajor])\n",
    "    tfListMajor_word.append(word)\n",
    "\n",
    "    \n",
    "## collecting specific words For Premise which are not in others\n",
    "tfListPremise =list()\n",
    "tfListPremise_word = list()\n",
    "for word,valuePremise in sortedPremiseDict.items():\n",
    "    \n",
    "    if word in sortedClaimDict:\n",
    "        valueClaim = sortedClaimDict[word]\n",
    "        if valueClaim >= valuePremise:\n",
    "            continue\n",
    "    elif word in sortedMajorDict:\n",
    "        valueMajor = sortedMajorDict[word]\n",
    "        if valueMajor >= valuePremise:\n",
    "            continue\n",
    "        \n",
    "    tfListPremise.append([word,valuePremise])\n",
    "    tfListPremise_word.append(word)\n",
    "\n",
    "## collecting specific words For Claims which are not in others\n",
    "tfListClaim =list()\n",
    "tfListClaim_word= list()\n",
    "for word,valueClaim in sortedClaimDict.items():\n",
    "\n",
    "    ##compare against major\n",
    "    if word in sortedMajorDict:\n",
    "        valueMajor = sortedMajorDict[word]\n",
    "        if valueMajor >= valueClaim:\n",
    "            continue\n",
    "            \n",
    "    ## compare against premises\n",
    "    elif word in sortedPremiseDict:\n",
    "        valuePremise = sortedPremiseDict[word]\n",
    "        if valuePremise >= valueClaim:\n",
    "            continue\n",
    "        \n",
    "    tfListClaim.append([word,valueClaim])\n",
    "    tfListClaim_word.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the most specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>majorClaim</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disagree</td>\n",
       "      <td>water</td>\n",
       "      <td>effectiveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scale</td>\n",
       "      <td>assignment</td>\n",
       "      <td>convict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allot</td>\n",
       "      <td>local</td>\n",
       "      <td>enable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compulsory</td>\n",
       "      <td>send</td>\n",
       "      <td>intellectual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fatherhood</td>\n",
       "      <td>favorite</td>\n",
       "      <td>means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>socioeconomic</td>\n",
       "      <td>crime</td>\n",
       "      <td>oppose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kidvid</td>\n",
       "      <td>report</td>\n",
       "      <td>broaden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>entirely</td>\n",
       "      <td>painting</td>\n",
       "      <td>dynamic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>maximum</td>\n",
       "      <td>pressure</td>\n",
       "      <td>passionate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>overemphasize</td>\n",
       "      <td>disease</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      majorClaim     Premise          Claim\n",
       "0       disagree       water  effectiveness\n",
       "1          scale  assignment        convict\n",
       "2          allot       local         enable\n",
       "3     compulsory        send   intellectual\n",
       "4     fatherhood    favorite          means\n",
       "5  socioeconomic       crime         oppose\n",
       "6         kidvid      report        broaden\n",
       "7       entirely    painting        dynamic\n",
       "8        maximum    pressure     passionate\n",
       "9  overemphasize     disease         mirror"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [tfListMajor_word[:10], tfListClaim_word[:10], tfListPremise_word[:10]]\n",
    "pd.DataFrame(list(zip(tfListMajor_word[:10],tfListPremise_word[:10],tfListClaim_word[:10])), columns = ['majorClaim', 'Premise','Claim'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
